<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PySpark on Imad Ali</title>
    <link>http://imadali.net/tags/pyspark/</link>
    <description>Recent content in PySpark on Imad Ali</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Sep 2020 16:14:19 -0700</lastBuildDate>
    
	<atom:link href="http://imadali.net/tags/pyspark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PySpark Application Execution</title>
      <link>http://imadali.net/posts/pyspark-application-execution/</link>
      <pubDate>Wed, 16 Sep 2020 16:14:19 -0700</pubDate>
      
      <guid>http://imadali.net/posts/pyspark-application-execution/</guid>
      <description>It took me a while to really understand how Spark (specifically PySpark) works under the hood. Specifically, I&amp;rsquo;m referring to what happens when I, as the user, submit a PySpark data processing job to a cluster. Even after all my digging around I&amp;rsquo;m still uncertain about how a few things work. I&amp;rsquo;m kinda of surprised at how convoluted the documentation is for a library/framework that&amp;rsquo;s so popular in industry. Nevertheless, this is my attempt at putting together a very high-level overview of how Spark works, from the perspective of the PySpark interface.</description>
    </item>
    
    <item>
      <title>PySpark UDFs</title>
      <link>http://imadali.net/posts/pyspark-udfs/</link>
      <pubDate>Tue, 15 Sep 2020 00:22:23 -0700</pubDate>
      
      <guid>http://imadali.net/posts/pyspark-udfs/</guid>
      <description>These are some of my notes on creating UDFs (user defined functions) in PySpark.
UDFs are super useful for anyone doing feature engineering or ETL work. They help break down the workflow by keeping your PySpark code modular. This makes it easy to perform unit testing (since you&amp;rsquo;re working with modular components that build up to the entire ETL workflow).
Here I show how to create a PySpark UDF which uses,</description>
    </item>
    
  </channel>
</rss>