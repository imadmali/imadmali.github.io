<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Imad Ali ">
<meta name="description" content="You can think of a neural network embedding is another form of dimensionality reduction. You&amp;rsquo;re taking a bunch of tokens (words, movies, games, etc) and instead of one-hot encoding them you want to map them down to a lower dimensional space.
For example, suppose you have a collection of 1,000 tokens. To one-hot encode them means having a very sparse vectors of length 1,000 for each word (where a 1 exists at the index that represents the token, 0 otherwise)." />
<meta name="keywords" content=", Deep Learning, Machine Learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="http://imadali.net/posts/embeddings/" />


    <title>
        
            Embeddings :: Imad Ali 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.min.75c31a52ad2c464af230bf9e46470cf42244ee17722c920ddf52f75a10d6b504.css">






<meta itemprop="name" content="Embeddings">
<meta itemprop="description" content="You can think of a neural network embedding is another form of dimensionality reduction. You&rsquo;re taking a bunch of tokens (words, movies, games, etc) and instead of one-hot encoding them you want to map them down to a lower dimensional space.
For example, suppose you have a collection of 1,000 tokens. To one-hot encode them means having a very sparse vectors of length 1,000 for each word (where a 1 exists at the index that represents the token, 0 otherwise).">
<meta itemprop="datePublished" content="2020-11-29T16:14:19-07:00" />
<meta itemprop="dateModified" content="2020-11-29T16:14:19-07:00" />
<meta itemprop="wordCount" content="654">



<meta itemprop="keywords" content="Deep Learning,Machine Learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Embeddings"/>
<meta name="twitter:description" content="You can think of a neural network embedding is another form of dimensionality reduction. You&rsquo;re taking a bunch of tokens (words, movies, games, etc) and instead of one-hot encoding them you want to map them down to a lower dimensional space.
For example, suppose you have a collection of 1,000 tokens. To one-hot encode them means having a very sparse vectors of length 1,000 for each word (where a 1 exists at the index that represents the token, 0 otherwise)."/>



    <meta property="og:title" content="Embeddings" />
<meta property="og:description" content="You can think of a neural network embedding is another form of dimensionality reduction. You&rsquo;re taking a bunch of tokens (words, movies, games, etc) and instead of one-hot encoding them you want to map them down to a lower dimensional space.
For example, suppose you have a collection of 1,000 tokens. To one-hot encode them means having a very sparse vectors of length 1,000 for each word (where a 1 exists at the index that represents the token, 0 otherwise)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://imadali.net/posts/embeddings/" />
<meta property="article:published_time" content="2020-11-29T16:14:19-07:00" />
<meta property="article:modified_time" content="2020-11-29T16:14:19-07:00" />






    <meta property="article:published_time" content="2020-11-29 16:14:19 -0700 -0700" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="http://imadali.net" style="text-decoration: none;">
    <div class="logo">
        
        
            <span class="logo__text">imad ali</span>
            <span class="logo__cursor" style=
                  "visibility:hidden;
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="http://imadali.net/posts/">Posts</a></li><li><a href="http://imadali.net/projects/">Projects</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <article>
            <h1 class="post-title">
                <a href="http://imadali.net/posts/embeddings/">Embeddings</a>
            </h1>

            <div class="post-info">
              <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-11-29
              </p>
              <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>4 minutes
              </p>
            </div>

            

            <div class="post-content">
                <p>You can think of a neural network embedding is another form of dimensionality reduction. You&rsquo;re taking a bunch of tokens (words, movies, games, etc) and instead of one-hot encoding them you want to map them down to a lower dimensional space.</p>
<p>For example, suppose you have a collection of 1,000 tokens. To one-hot encode them means having a very sparse vectors of length 1,000 for each word (where a 1 exists at the index that represents the token, 0 otherwise).</p>
<p>So basically you&rsquo;re taking very high dimensional vectors and mapping them down to a low dimensional space. Preferably you don&rsquo;t want the low dimensional space to be random. You want it to have some structure. For example, vectors that represent similar tokens are close to one another, adding vectors together gets you close to a vector that represents a good mix of the two tokens, etc).</p>
<p>I like using generated (synthetic) data. It gives you a good understanding of how the data you&rsquo;re modeling was generated. Below I show how to discover an embedding space from some generated data and and simple predictive visualization.</p>
<h1 id="generating-data">Generating Data</h1>
<p>Using numpy we generate some data that we can create embeddings with. Suppose we have 5 tokens that have associated effects (i.e. parameters that we can use to generate the data from). Using the tokens and the token effects we generate 10,000 observations from the normal distribution.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">N <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>_000
n_tokens <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
token_effect <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=-</span><span style="color:#ae81ff">20</span>, stop<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, num<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
<span style="color:#75715e"># reorder the tokens (not necessary but more realistic)</span>
token_effect <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(token_effect, n_tokens, replace<span style="color:#f92672">=</span>False)
<span style="color:#75715e"># generate tokens for each observation</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(n_tokens, N)
<span style="color:#75715e"># simulate an outcome for each observation&#39;s token</span>
y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(token_effect[x], scale<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, size<span style="color:#f92672">=</span>N)
</code></pre></div><p>The tokens <code>x[:5]</code> might look something like <code>array([4, 4, 3, 2, 2, 0])</code> and the outcome <code>y[:5]</code> might look something like <code>array([ 18.64902242,  19.22041727, -20.25015504,  11.30246504, 11.15165945, -10.05891201])</code>.</p>
<h1 id="creating-embeddings">Creating Embeddings</h1>
<p>Knowing how many tokens we have and what we want our embedding space to look like we can create our embedding model with keras. We define the dimensionality of the embedding layer with the number of unique tokens and the output dimension (which should be substantially less than the number of tokens).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential()
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Embedding(input_dim <span style="color:#f92672">=</span> n_tokens, output_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>, input_length<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>))
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>))
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>))
model<span style="color:#f92672">.</span>add(layers<span style="color:#f92672">.</span>Dense(units<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>))
model<span style="color:#f92672">.</span>summary()
</code></pre></div><p>Now we compile/train the model and plot the loss to confirm that our model configuration (choice of layers, activation functions, loss function, etc) is appropriate.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rmsprop&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>)
history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>x, y<span style="color:#f92672">=</span>y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>_000, validation_split<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</code></pre></div><img src="/images/embeddings/loss.svg" class="center"/>
<h1 id="embedding-similarity">Embedding Similarity</h1>
<p>Our token effect might look something like <code>array([-10., 0., 10., 20., -20.])</code>. In this case we would expect token 3 (which maps to a token effect of 20) to be closest to token 2 (which maps to a token effect of 10) and farthest from token 4 (which maps to a token effect of -20).</p>
<p>The function below computes the similarity (Euclidean distance) for an indexed embedding vector against all the other embedding vectors. <code>np.argmax(token_effect)</code> gives us the token value that maps to the largest token effect.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">similarities</span>(target_index, embedding_matrix):
    result <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,embedding_matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
        s <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(embedding_matrix[target_index,] <span style="color:#f92672">-</span> embedding_matrix[i,])
        result<span style="color:#f92672">.</span>append(s)
    <span style="color:#66d9ef">return</span>(np<span style="color:#f92672">.</span>array(result))

similarities(np<span style="color:#f92672">.</span>argmax(token_effect), embedding)
</code></pre></div><p>The above might return similarity values like <code>array([0.7304573 , 0.5255725 , 0.32273838, 0. , 1.0848211 ]</code> which confirm that the embedding corresponding to token 3 is closest to token 2 and farthest from token 4.</p>
<p>This is a convenient result. For illustrative purposes we used a small number of tokens. But you can see how calculating similarities between vectors of an embedding matrix would be preferred to the one-hot encoded counterpart in situations where you might have thousands of tokens.</p>
<h1 id="visualizing-predictions">Visualizing Predictions</h1>
<p>Here&rsquo;s what the outcome variables looks like, color coded by token.</p>
<img src="/images/embeddings/outcome.svg" class="center"/>
<p>And here are the predictions using the model along with the outcome variable. This plot doesn&rsquo;t really provide any new information. It just reiterates the fact that the loss from training is sufficiently low.</p>
<img src="/images/embeddings/pred-check.svg" class="center"/>

            </div>
        </article>

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="http://imadali.net/tags/deep-learning">Deep Learning</a></span><span class="tag"><a href="http://imadali.net/tags/machine-learning">Machine Learning</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>654 Words</p>
        </div>

        
    </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy 2020 Imad Ali</span>
            <span>Powered by <a href="http://gohugo.io">Hugo</a> and <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
        </div>
    </div>
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload='renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    });'></script>

    
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.4a69500057d68129e88f497d354afe68422eb56de6d15e45dbe2190858ea5a76bfcb096406f992984b241db45f47388ac57ab0376e3b32125bef7a8a6d0f06c4.js" integrity="sha512-SmlQAFfWgSnoj0l9NUr&#43;aEIutW3m0V5F2&#43;IZCFjqWna/ywlkBvmSmEskHbRfRziKxXqwN247MhJb73qKbQ8GxA=="></script>



    </body>
</html>
